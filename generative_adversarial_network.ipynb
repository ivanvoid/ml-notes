{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loading necessary libraries\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Setup parameters\n",
    "'''\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "n_epochs = 50\n",
    "n_classes = 10\n",
    "batch_size = 100\n",
    "lr = 2e-4\n",
    "\n",
    "latent_size = 64\n",
    "hidden_size = 256\n",
    "image_size=784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loading MNIST dataset\n",
    "'''\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])]) # [0.5,0.5,0.5] for color image\n",
    "\n",
    "# Datasets\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data/mnist', \n",
    "                                           train=True,\n",
    "                                           download=True, \n",
    "                                           transform=transform)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data/mnist', \n",
    "                                          train=False,\n",
    "                                          download=True, \n",
    "                                          transform=transform)\n",
    "\n",
    "# Loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True, \n",
    "                                           num_workers=12)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False, \n",
    "                                          num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define model\n",
    "'''\n",
    "Discriminator = nn.Sequential(\n",
    "    nn.Linear(image_size, hidden_size),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(hidden_size, 1),\n",
    "    nn.Sigmoid())\n",
    "    \n",
    "Generator = nn.Sequential(\n",
    "    nn.Linear(latent_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, image_size),\n",
    "    nn.Tanh())\n",
    "\n",
    "Discriminator = Discriminator.to(device)\n",
    "Generator = Generator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Optimizer and Loss function\n",
    "'''\n",
    "# Binary cross entropy \n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "optimizer_d = torch.optim.Adam(Discriminator.parameters(), lr=lr)\n",
    "optimizer_g = torch.optim.Adam(Generator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]-[200/600], D-loss: 0.0402, G-loss: 4.2268, D(x):0.99, D(G(z)):0.0341\n",
      "Epoch [1/50]-[400/600], D-loss: 0.0494, G-loss: 7.4043, D(x):0.98, D(G(z)):0.0161\n",
      "Epoch [1/50]-[600/600], D-loss: 0.0756, G-loss: 4.3754, D(x):0.98, D(G(z)):0.0562\n",
      "Epoch [2/50]-[200/600], D-loss: 0.0831, G-loss: 4.7943, D(x):0.97, D(G(z)):0.0471\n",
      "Epoch [2/50]-[400/600], D-loss: 0.4232, G-loss: 3.5036, D(x):0.88, D(G(z)):0.1753\n",
      "Epoch [2/50]-[600/600], D-loss: 0.7741, G-loss: 3.2409, D(x):0.85, D(G(z)):0.3315\n",
      "Epoch [3/50]-[200/600], D-loss: 0.0668, G-loss: 4.7927, D(x):0.96, D(G(z)):0.0243\n",
      "Epoch [3/50]-[400/600], D-loss: 0.8147, G-loss: 2.0243, D(x):0.69, D(G(z)):0.2021\n",
      "Epoch [3/50]-[600/600], D-loss: 0.5705, G-loss: 2.7803, D(x):0.75, D(G(z)):0.1497\n",
      "Epoch [4/50]-[200/600], D-loss: 0.2388, G-loss: 3.3218, D(x):0.91, D(G(z)):0.0768\n",
      "Epoch [4/50]-[400/600], D-loss: 0.6129, G-loss: 2.0436, D(x):0.85, D(G(z)):0.2534\n",
      "Epoch [4/50]-[600/600], D-loss: 0.8376, G-loss: 2.4268, D(x):0.76, D(G(z)):0.2542\n",
      "Epoch [5/50]-[200/600], D-loss: 1.0491, G-loss: 1.7433, D(x):0.77, D(G(z)):0.3644\n",
      "Epoch [5/50]-[400/600], D-loss: 1.3162, G-loss: 1.7682, D(x):0.65, D(G(z)):0.4226\n",
      "Epoch [5/50]-[600/600], D-loss: 0.5076, G-loss: 2.3304, D(x):0.81, D(G(z)):0.1853\n",
      "Epoch [6/50]-[200/600], D-loss: 0.8754, G-loss: 2.0763, D(x):0.86, D(G(z)):0.4223\n",
      "Epoch [6/50]-[400/600], D-loss: 0.2422, G-loss: 3.5788, D(x):0.91, D(G(z)):0.1039\n",
      "Epoch [6/50]-[600/600], D-loss: 0.3936, G-loss: 2.7467, D(x):0.90, D(G(z)):0.1733\n",
      "Epoch [7/50]-[200/600], D-loss: 0.1420, G-loss: 3.7918, D(x):0.94, D(G(z)):0.0373\n",
      "Epoch [7/50]-[400/600], D-loss: 0.5434, G-loss: 2.8701, D(x):0.81, D(G(z)):0.2035\n",
      "Epoch [7/50]-[600/600], D-loss: 0.7674, G-loss: 2.1246, D(x):0.79, D(G(z)):0.2258\n",
      "Epoch [8/50]-[200/600], D-loss: 0.2553, G-loss: 3.3660, D(x):0.95, D(G(z)):0.1678\n",
      "Epoch [8/50]-[400/600], D-loss: 0.3070, G-loss: 2.3636, D(x):0.95, D(G(z)):0.1719\n",
      "Epoch [8/50]-[600/600], D-loss: 0.8250, G-loss: 2.6296, D(x):0.78, D(G(z)):0.2048\n",
      "Epoch [9/50]-[200/600], D-loss: 0.1668, G-loss: 4.3214, D(x):0.93, D(G(z)):0.0609\n",
      "Epoch [9/50]-[400/600], D-loss: 0.3183, G-loss: 4.0481, D(x):0.92, D(G(z)):0.1497\n",
      "Epoch [9/50]-[600/600], D-loss: 0.1899, G-loss: 3.9143, D(x):0.94, D(G(z)):0.0541\n",
      "Epoch [10/50]-[200/600], D-loss: 0.2507, G-loss: 5.5827, D(x):0.92, D(G(z)):0.0134\n",
      "Epoch [10/50]-[400/600], D-loss: 0.1023, G-loss: 3.4265, D(x):0.97, D(G(z)):0.0633\n",
      "Epoch [10/50]-[600/600], D-loss: 0.1144, G-loss: 5.5029, D(x):0.95, D(G(z)):0.0194\n",
      "Epoch [11/50]-[200/600], D-loss: 0.1272, G-loss: 8.3534, D(x):0.98, D(G(z)):0.0216\n",
      "Epoch [11/50]-[400/600], D-loss: 0.0486, G-loss: 5.7081, D(x):0.98, D(G(z)):0.0207\n",
      "Epoch [11/50]-[600/600], D-loss: 0.0475, G-loss: 5.4154, D(x):0.99, D(G(z)):0.0316\n",
      "Epoch [12/50]-[200/600], D-loss: 0.1139, G-loss: 6.7819, D(x):0.97, D(G(z)):0.0140\n",
      "Epoch [12/50]-[400/600], D-loss: 0.1470, G-loss: 4.6033, D(x):0.98, D(G(z)):0.0848\n",
      "Epoch [12/50]-[600/600], D-loss: 0.1693, G-loss: 6.9347, D(x):0.98, D(G(z)):0.1126\n",
      "Epoch [13/50]-[200/600], D-loss: 0.0910, G-loss: 5.5346, D(x):0.97, D(G(z)):0.0359\n",
      "Epoch [13/50]-[400/600], D-loss: 0.1924, G-loss: 5.7946, D(x):0.94, D(G(z)):0.0228\n",
      "Epoch [13/50]-[600/600], D-loss: 0.2602, G-loss: 4.0334, D(x):0.94, D(G(z)):0.0914\n",
      "Epoch [14/50]-[200/600], D-loss: 0.1893, G-loss: 4.4957, D(x):0.93, D(G(z)):0.0242\n",
      "Epoch [14/50]-[400/600], D-loss: 0.1778, G-loss: 4.6395, D(x):0.98, D(G(z)):0.1151\n",
      "Epoch [14/50]-[600/600], D-loss: 0.0786, G-loss: 5.2309, D(x):0.99, D(G(z)):0.0591\n",
      "Epoch [15/50]-[200/600], D-loss: 0.2524, G-loss: 5.6980, D(x):0.93, D(G(z)):0.0073\n",
      "Epoch [15/50]-[400/600], D-loss: 0.0953, G-loss: 5.6507, D(x):0.98, D(G(z)):0.0607\n",
      "Epoch [15/50]-[600/600], D-loss: 0.1421, G-loss: 6.2608, D(x):0.96, D(G(z)):0.0424\n",
      "Epoch [16/50]-[200/600], D-loss: 0.1059, G-loss: 5.3529, D(x):0.98, D(G(z)):0.0598\n",
      "Epoch [16/50]-[400/600], D-loss: 0.1725, G-loss: 6.3398, D(x):0.94, D(G(z)):0.0187\n",
      "Epoch [16/50]-[600/600], D-loss: 0.2178, G-loss: 5.4051, D(x):0.98, D(G(z)):0.1313\n",
      "Epoch [17/50]-[200/600], D-loss: 0.2888, G-loss: 4.2825, D(x):0.93, D(G(z)):0.0889\n",
      "Epoch [17/50]-[400/600], D-loss: 0.1821, G-loss: 5.6412, D(x):0.94, D(G(z)):0.0298\n",
      "Epoch [17/50]-[600/600], D-loss: 0.1689, G-loss: 4.2350, D(x):0.98, D(G(z)):0.1151\n",
      "Epoch [18/50]-[200/600], D-loss: 0.1230, G-loss: 6.2285, D(x):0.96, D(G(z)):0.0407\n",
      "Epoch [18/50]-[400/600], D-loss: 0.1519, G-loss: 7.0172, D(x):0.96, D(G(z)):0.0219\n",
      "Epoch [18/50]-[600/600], D-loss: 0.2235, G-loss: 5.1040, D(x):0.95, D(G(z)):0.0815\n",
      "Epoch [19/50]-[200/600], D-loss: 0.1522, G-loss: 4.5884, D(x):0.95, D(G(z)):0.0271\n",
      "Epoch [19/50]-[400/600], D-loss: 0.1241, G-loss: 5.6893, D(x):0.94, D(G(z)):0.0166\n",
      "Epoch [19/50]-[600/600], D-loss: 0.4787, G-loss: 8.8494, D(x):0.86, D(G(z)):0.0153\n",
      "Epoch [20/50]-[200/600], D-loss: 0.4741, G-loss: 5.3370, D(x):0.88, D(G(z)):0.0421\n",
      "Epoch [20/50]-[400/600], D-loss: 0.1576, G-loss: 5.3420, D(x):0.93, D(G(z)):0.0237\n",
      "Epoch [20/50]-[600/600], D-loss: 0.1108, G-loss: 6.6120, D(x):0.96, D(G(z)):0.0301\n",
      "Epoch [21/50]-[200/600], D-loss: 0.3111, G-loss: 5.5964, D(x):0.97, D(G(z)):0.1127\n",
      "Epoch [21/50]-[400/600], D-loss: 0.1754, G-loss: 5.0247, D(x):0.95, D(G(z)):0.0636\n",
      "Epoch [21/50]-[600/600], D-loss: 0.4982, G-loss: 3.4256, D(x):0.84, D(G(z)):0.0752\n",
      "Epoch [22/50]-[200/600], D-loss: 0.3764, G-loss: 4.1567, D(x):0.95, D(G(z)):0.1566\n",
      "Epoch [22/50]-[400/600], D-loss: 0.2688, G-loss: 4.9743, D(x):0.92, D(G(z)):0.0629\n",
      "Epoch [22/50]-[600/600], D-loss: 0.4729, G-loss: 5.1831, D(x):0.97, D(G(z)):0.2397\n",
      "Epoch [23/50]-[200/600], D-loss: 0.2103, G-loss: 5.1240, D(x):0.97, D(G(z)):0.0781\n",
      "Epoch [23/50]-[400/600], D-loss: 0.3381, G-loss: 5.3646, D(x):0.90, D(G(z)):0.0351\n",
      "Epoch [23/50]-[600/600], D-loss: 0.2226, G-loss: 4.9083, D(x):0.91, D(G(z)):0.0335\n",
      "Epoch [24/50]-[200/600], D-loss: 0.3021, G-loss: 4.5946, D(x):0.88, D(G(z)):0.0294\n",
      "Epoch [24/50]-[400/600], D-loss: 0.1681, G-loss: 5.3794, D(x):0.94, D(G(z)):0.0355\n",
      "Epoch [24/50]-[600/600], D-loss: 0.4074, G-loss: 5.2606, D(x):0.90, D(G(z)):0.0823\n",
      "Epoch [25/50]-[200/600], D-loss: 0.2427, G-loss: 4.2095, D(x):0.92, D(G(z)):0.0545\n",
      "Epoch [25/50]-[400/600], D-loss: 0.2240, G-loss: 5.3300, D(x):0.94, D(G(z)):0.0693\n",
      "Epoch [25/50]-[600/600], D-loss: 0.1001, G-loss: 4.6702, D(x):0.97, D(G(z)):0.0451\n",
      "Epoch [26/50]-[200/600], D-loss: 0.2119, G-loss: 4.3970, D(x):0.94, D(G(z)):0.0908\n",
      "Epoch [26/50]-[400/600], D-loss: 0.2697, G-loss: 3.7799, D(x):0.94, D(G(z)):0.0981\n",
      "Epoch [26/50]-[600/600], D-loss: 0.2452, G-loss: 5.2275, D(x):0.92, D(G(z)):0.0486\n",
      "Epoch [27/50]-[200/600], D-loss: 0.1300, G-loss: 6.4626, D(x):0.97, D(G(z)):0.0626\n",
      "Epoch [27/50]-[400/600], D-loss: 0.0936, G-loss: 4.4983, D(x):0.97, D(G(z)):0.0319\n",
      "Epoch [27/50]-[600/600], D-loss: 0.2156, G-loss: 5.5156, D(x):0.98, D(G(z)):0.1150\n",
      "Epoch [28/50]-[200/600], D-loss: 0.2685, G-loss: 4.5645, D(x):0.93, D(G(z)):0.0800\n",
      "Epoch [28/50]-[400/600], D-loss: 0.0935, G-loss: 6.2322, D(x):0.98, D(G(z)):0.0273\n",
      "Epoch [28/50]-[600/600], D-loss: 0.2357, G-loss: 5.8349, D(x):0.91, D(G(z)):0.0196\n",
      "Epoch [29/50]-[200/600], D-loss: 0.0895, G-loss: 5.1984, D(x):0.97, D(G(z)):0.0426\n",
      "Epoch [29/50]-[400/600], D-loss: 0.0994, G-loss: 4.9482, D(x):0.97, D(G(z)):0.0481\n",
      "Epoch [29/50]-[600/600], D-loss: 0.1842, G-loss: 4.3908, D(x):0.96, D(G(z)):0.0705\n",
      "Epoch [30/50]-[200/600], D-loss: 0.2419, G-loss: 4.9697, D(x):0.90, D(G(z)):0.0472\n",
      "Epoch [30/50]-[400/600], D-loss: 0.3568, G-loss: 5.1215, D(x):0.86, D(G(z)):0.0279\n",
      "Epoch [30/50]-[600/600], D-loss: 0.2853, G-loss: 5.3730, D(x):0.91, D(G(z)):0.0531\n",
      "Epoch [31/50]-[200/600], D-loss: 0.0954, G-loss: 6.1429, D(x):0.96, D(G(z)):0.0303\n",
      "Epoch [31/50]-[400/600], D-loss: 0.3308, G-loss: 5.1097, D(x):0.96, D(G(z)):0.1636\n",
      "Epoch [31/50]-[600/600], D-loss: 0.3215, G-loss: 5.4125, D(x):0.93, D(G(z)):0.0878\n",
      "Epoch [32/50]-[200/600], D-loss: 0.3375, G-loss: 4.7615, D(x):0.90, D(G(z)):0.1027\n",
      "Epoch [32/50]-[400/600], D-loss: 0.4451, G-loss: 4.1233, D(x):0.85, D(G(z)):0.0484\n",
      "Epoch [32/50]-[600/600], D-loss: 0.1986, G-loss: 4.0702, D(x):0.94, D(G(z)):0.0744\n",
      "Epoch [33/50]-[200/600], D-loss: 0.1577, G-loss: 3.8983, D(x):0.96, D(G(z)):0.0777\n",
      "Epoch [33/50]-[400/600], D-loss: 0.3191, G-loss: 5.7901, D(x):0.86, D(G(z)):0.0167\n",
      "Epoch [33/50]-[600/600], D-loss: 0.1772, G-loss: 3.5168, D(x):0.96, D(G(z)):0.0842\n",
      "Epoch [34/50]-[200/600], D-loss: 0.3440, G-loss: 4.6836, D(x):0.89, D(G(z)):0.1030\n",
      "Epoch [34/50]-[400/600], D-loss: 0.4450, G-loss: 3.3868, D(x):0.95, D(G(z)):0.2196\n",
      "Epoch [34/50]-[600/600], D-loss: 0.2232, G-loss: 4.1145, D(x):0.93, D(G(z)):0.0639\n",
      "Epoch [35/50]-[200/600], D-loss: 0.3142, G-loss: 3.9735, D(x):0.94, D(G(z)):0.1428\n",
      "Epoch [35/50]-[400/600], D-loss: 0.3563, G-loss: 4.8381, D(x):0.89, D(G(z)):0.0744\n",
      "Epoch [35/50]-[600/600], D-loss: 0.2126, G-loss: 4.3777, D(x):0.88, D(G(z)):0.0247\n",
      "Epoch [36/50]-[200/600], D-loss: 0.2198, G-loss: 3.3711, D(x):0.96, D(G(z)):0.1399\n",
      "Epoch [36/50]-[400/600], D-loss: 0.4125, G-loss: 4.7460, D(x):0.84, D(G(z)):0.0612\n",
      "Epoch [36/50]-[600/600], D-loss: 0.2713, G-loss: 4.7006, D(x):0.93, D(G(z)):0.1202\n",
      "Epoch [37/50]-[200/600], D-loss: 0.3467, G-loss: 4.8645, D(x):0.91, D(G(z)):0.1054\n",
      "Epoch [37/50]-[400/600], D-loss: 0.3831, G-loss: 4.3531, D(x):0.90, D(G(z)):0.1595\n",
      "Epoch [37/50]-[600/600], D-loss: 0.2592, G-loss: 2.9943, D(x):0.92, D(G(z)):0.1129\n",
      "Epoch [38/50]-[200/600], D-loss: 0.3180, G-loss: 4.5259, D(x):0.86, D(G(z)):0.0635\n",
      "Epoch [38/50]-[400/600], D-loss: 0.5866, G-loss: 3.9962, D(x):0.81, D(G(z)):0.0791\n",
      "Epoch [38/50]-[600/600], D-loss: 0.3644, G-loss: 3.9188, D(x):0.89, D(G(z)):0.1073\n",
      "Epoch [39/50]-[200/600], D-loss: 0.2749, G-loss: 2.6289, D(x):0.90, D(G(z)):0.0928\n",
      "Epoch [39/50]-[400/600], D-loss: 0.4636, G-loss: 3.6217, D(x):0.86, D(G(z)):0.1238\n",
      "Epoch [39/50]-[600/600], D-loss: 0.4753, G-loss: 3.7652, D(x):0.87, D(G(z)):0.1760\n",
      "Epoch [40/50]-[200/600], D-loss: 0.5656, G-loss: 2.3535, D(x):0.86, D(G(z)):0.1701\n",
      "Epoch [40/50]-[400/600], D-loss: 0.3418, G-loss: 3.4953, D(x):0.91, D(G(z)):0.1521\n",
      "Epoch [40/50]-[600/600], D-loss: 0.3323, G-loss: 3.8934, D(x):0.90, D(G(z)):0.1215\n",
      "Epoch [41/50]-[200/600], D-loss: 0.5305, G-loss: 3.5863, D(x):0.85, D(G(z)):0.1665\n",
      "Epoch [41/50]-[400/600], D-loss: 0.4135, G-loss: 3.5116, D(x):0.93, D(G(z)):0.2054\n",
      "Epoch [41/50]-[600/600], D-loss: 0.4052, G-loss: 4.0698, D(x):0.82, D(G(z)):0.0508\n",
      "Epoch [42/50]-[200/600], D-loss: 0.5223, G-loss: 2.8615, D(x):0.97, D(G(z)):0.2667\n",
      "Epoch [42/50]-[400/600], D-loss: 0.2938, G-loss: 3.6057, D(x):0.92, D(G(z)):0.1355\n",
      "Epoch [42/50]-[600/600], D-loss: 0.5692, G-loss: 2.5207, D(x):0.88, D(G(z)):0.2314\n",
      "Epoch [43/50]-[200/600], D-loss: 0.5024, G-loss: 4.2982, D(x):0.81, D(G(z)):0.0987\n",
      "Epoch [43/50]-[400/600], D-loss: 0.3753, G-loss: 3.0856, D(x):0.89, D(G(z)):0.1441\n",
      "Epoch [43/50]-[600/600], D-loss: 0.3917, G-loss: 4.3903, D(x):0.88, D(G(z)):0.1230\n",
      "Epoch [44/50]-[200/600], D-loss: 0.4740, G-loss: 4.1591, D(x):0.86, D(G(z)):0.1460\n",
      "Epoch [44/50]-[400/600], D-loss: 0.4378, G-loss: 3.8970, D(x):0.93, D(G(z)):0.1894\n",
      "Epoch [44/50]-[600/600], D-loss: 0.2781, G-loss: 4.4783, D(x):0.89, D(G(z)):0.0728\n",
      "Epoch [45/50]-[200/600], D-loss: 0.3563, G-loss: 4.5632, D(x):0.88, D(G(z)):0.1288\n",
      "Epoch [45/50]-[400/600], D-loss: 0.4151, G-loss: 4.6804, D(x):0.85, D(G(z)):0.0925\n",
      "Epoch [45/50]-[600/600], D-loss: 0.1861, G-loss: 4.2498, D(x):0.93, D(G(z)):0.0707\n",
      "Epoch [46/50]-[200/600], D-loss: 0.3223, G-loss: 3.8279, D(x):0.92, D(G(z)):0.1542\n",
      "Epoch [46/50]-[400/600], D-loss: 0.5590, G-loss: 3.8851, D(x):0.84, D(G(z)):0.1744\n",
      "Epoch [46/50]-[600/600], D-loss: 0.4331, G-loss: 4.2683, D(x):0.82, D(G(z)):0.0871\n",
      "Epoch [47/50]-[200/600], D-loss: 0.5522, G-loss: 4.1002, D(x):0.84, D(G(z)):0.1921\n",
      "Epoch [47/50]-[400/600], D-loss: 0.3136, G-loss: 4.0449, D(x):0.85, D(G(z)):0.0752\n",
      "Epoch [47/50]-[600/600], D-loss: 0.4472, G-loss: 2.6634, D(x):0.89, D(G(z)):0.1754\n",
      "Epoch [48/50]-[200/600], D-loss: 0.5974, G-loss: 2.1839, D(x):0.80, D(G(z)):0.1434\n",
      "Epoch [48/50]-[400/600], D-loss: 0.5226, G-loss: 3.5504, D(x):0.77, D(G(z)):0.0513\n",
      "Epoch [48/50]-[600/600], D-loss: 0.7500, G-loss: 2.4872, D(x):0.84, D(G(z)):0.2930\n",
      "Epoch [49/50]-[200/600], D-loss: 0.4747, G-loss: 2.5646, D(x):0.79, D(G(z)):0.0728\n",
      "Epoch [49/50]-[400/600], D-loss: 0.5634, G-loss: 2.6389, D(x):0.85, D(G(z)):0.1773\n",
      "Epoch [49/50]-[600/600], D-loss: 0.5719, G-loss: 2.7646, D(x):0.77, D(G(z)):0.0883\n",
      "Epoch [50/50]-[200/600], D-loss: 0.7083, G-loss: 3.4355, D(x):0.75, D(G(z)):0.1074\n",
      "Epoch [50/50]-[400/600], D-loss: 0.5169, G-loss: 3.2422, D(x):0.86, D(G(z)):0.1993\n",
      "Epoch [50/50]-[600/600], D-loss: 0.5884, G-loss: 2.6681, D(x):0.81, D(G(z)):0.1616\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Train the model\n",
    "'''\n",
    "# def train_discriminator():\n",
    "# def train_generator():\n",
    "\n",
    "total_steps = len(train_loader)\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (images, _) in enumerate(train_loader):\n",
    "        images = images.reshape(batch_size, -1).to(device)\n",
    "        \n",
    "        # Create labels for the BCE loss\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "        \n",
    "        # Train Discriminator\n",
    "        # Compute BCE loss using real images\n",
    "        output = Discriminator(images)\n",
    "        loss_d_real = loss_fn(output, real_labels)\n",
    "        real_score = output\n",
    "        \n",
    "        # Compute BCE lossusing fake images\n",
    "        z = torch.randn(batch_size, latent_size).to(device)\n",
    "        fake_images = Generator(z)\n",
    "        output = Discriminator(fake_images)\n",
    "        loss_d_fake = loss_fn(output, fake_labels)\n",
    "        fake_score = output\n",
    "        \n",
    "        # backprop\n",
    "        loss_d = loss_d_real + loss_d_fake\n",
    "        optimizer_d.zero_grad()\n",
    "        optimizer_g.zero_grad()\n",
    "        loss_d.backward()\n",
    "        optimizer_d.step()\n",
    "        \n",
    "        # Train Generator\n",
    "        # Compute loss for fake images\n",
    "        z = torch.randn(batch_size, latent_size).to(device)\n",
    "        fake_images = Generator(z)\n",
    "        output = Discriminator(fake_images)\n",
    "        \n",
    "        # We train G to maximize log(D(G(z)) instead of minimizing log(1-D(G(z)))\n",
    "        # For the reason, see the last paragraph of section 3. https://arxiv.org/pdf/1406.2661.pdf\n",
    "        loss_g = loss_fn(output, real_labels)\n",
    "        \n",
    "        # backprop\n",
    "        optimizer_d.zero_grad()\n",
    "        optimizer_g.zero_grad()\n",
    "        loss_g.backward()\n",
    "        optimizer_g.step()\n",
    "        \n",
    "        if (i+1)%200==0:\n",
    "            print('Epoch [{}/{}]-[{}/{}], D-loss: {:.4f}, G-loss: {:.4f}, D(x):{:.2f}, D(G(z)):{:.4f}'\n",
    "                 .format(epoch+1, n_epochs, i+1, total_steps, loss_d.item(), loss_g.item(),\n",
    "                        real_score.mean().item(), fake_score.mean().item()))\n",
    "    # Save fake images\n",
    "    fake_images = fake_images.reshape(fake_images.size(0), 1, 28, 28)\n",
    "    save_image(denorm(fake_images), './fake_images/fake-image-'+str(epoch+1)+'.png')\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
